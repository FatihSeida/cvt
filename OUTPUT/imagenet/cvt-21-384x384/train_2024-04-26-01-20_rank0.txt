2024-04-26 01:20:58,356:[P:8463]:Rank[0/2] => collecting env info (might take some time)
2024-04-26 01:20:59,535:[P:8463]:Rank[0/2] 
PyTorch version: 2.3.0+cu121
Is debug build: False
CUDA used to build PyTorch: 12.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.3 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: 14.0.0-1ubuntu1.1
CMake version: version 3.27.9
Libc version: glibc-2.35

Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-6.1.58+-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.2.140
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA L4
Nvidia driver version: 535.104.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                       x86_64
CPU op-mode(s):                     32-bit, 64-bit
Address sizes:                      46 bits physical, 48 bits virtual
Byte Order:                         Little Endian
CPU(s):                             16
On-line CPU(s) list:                0-15
Vendor ID:                          GenuineIntel
Model name:                         Intel(R) Xeon(R) CPU @ 2.20GHz
CPU family:                         6
Model:                              85
Thread(s) per core:                 2
Core(s) per socket:                 8
Socket(s):                          1
Stepping:                           7
BogoMIPS:                           4400.46
Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities
Hypervisor vendor:                  KVM
Virtualization type:                full
L1d cache:                          256 KiB (8 instances)
L1i cache:                          256 KiB (8 instances)
L2 cache:                           8 MiB (8 instances)
L3 cache:                           38.5 MiB (1 instance)
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-15
Vulnerability Gather data sampling: Not affected
Vulnerability Itlb multihit:        Not affected
Vulnerability L1tf:                 Not affected
Vulnerability Mds:                  Vulnerable; SMT Host state unknown
Vulnerability Meltdown:             Not affected
Vulnerability Mmio stale data:      Vulnerable
Vulnerability Retbleed:             Vulnerable
Vulnerability Spec rstack overflow: Not affected
Vulnerability Spec store bypass:    Vulnerable
Vulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable
Vulnerability Srbds:                Not affected
Vulnerability Tsx async abort:      Vulnerable

Versions of relevant libraries:
[pip3] numpy==1.25.2
[pip3] torch==2.3.0
[pip3] torchaudio==2.3.0
[pip3] torchdata==0.7.1
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.17.1
[pip3] torchvision==0.18.0
[pip3] triton==2.3.0
[conda] Could not collect
2024-04-26 01:20:59,536:[P:8463]:Rank[0/2] Namespace(cfg='experiments/imagenet/cvt/cvt-21-384x384.yaml', local_rank=0, port=9000, opts=[], num_gpus=2, distributed=True)
2024-04-26 01:20:59,536:[P:8463]:Rank[0/2] AMP:
  ENABLED: True
  MEMORY_FORMAT: nchw
AUG:
  COLOR_JITTER: [0.4, 0.4, 0.4, 0.1, 0.0]
  DROPBLOCK_BLOCK_SIZE: 7
  DROPBLOCK_KEEP_PROB: 1.0
  DROPBLOCK_LAYERS: [3, 4]
  GAUSSIAN_BLUR: 0.0
  GRAY_SCALE: 0.0
  INTERPOLATION: 2
  MIXCUT: 1.0
  MIXCUT_AND_MIXUP: False
  MIXCUT_MINMAX: []
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RATIO: (0.75, 1.3333333333333333)
  SCALE: (0.08, 1.0)
  TIMM_AUG:
    AUTO_AUGMENT: rand-m9-mstd0.5-inc1
    COLOR_JITTER: 0.4
    HFLIP: 0.5
    INTERPOLATION: bicubic
    RE_COUNT: 1
    RE_MODE: pixel
    RE_PROB: 0.25
    RE_SPLIT: False
    USE_LOADER: False
    USE_TRANSFORM: True
    VFLIP: 0.0
BASE: ['']
CUDNN:
  BENCHMARK: True
  DETERMINISTIC: False
  ENABLED: True
DATASET:
  DATASET: imagenet
  DATA_FORMAT: jpg
  LABELMAP: 
  ROOT: /content/drive/My Drive/DATASET/imagenet
  SAMPLER: default
  TARGET_SIZE: -1
  TEST_SET: val
  TEST_TSV_LIST: []
  TRAIN_SET: train
  TRAIN_TSV_LIST: []
DATA_DIR: 
DEBUG:
  DEBUG: False
DIST_BACKEND: nccl
FINETUNE:
  BASE_LR: 0.003
  BATCH_SIZE: 512
  EVAL_EVERY: 3000
  FINETUNE: False
  FROZEN_LAYERS: []
  LR_SCHEDULER:
    DECAY_TYPE: step
  TRAIN_MODE: True
  USE_TRAIN_AUG: True
GPUS: (0,)
INPUT:
  MEAN: [0.485, 0.456, 0.406]
  STD: [0.229, 0.224, 0.225]
LOSS:
  LABEL_SMOOTHING: 0.1
  LOSS: softmax
MODEL:
  INIT_WEIGHTS: True
  NAME: cls_cvt
  NUM_CLASSES: 1000
  PRETRAINED: 
  PRETRAINED_LAYERS: ['*']
  SPEC:
    ATTN_DROP_RATE: [0.0, 0.0, 0.0]
    CLS_TOKEN: [False, False, True]
    DEPTH: [1, 4, 16]
    DIM_EMBED: [64, 192, 384]
    DROP_PATH_RATE: [0.0, 0.0, 0.1]
    DROP_RATE: [0.0, 0.0, 0.0]
    INIT: trunc_norm
    KERNEL_QKV: [3, 3, 3]
    MLP_RATIO: [4.0, 4.0, 4.0]
    NUM_HEADS: [1, 3, 6]
    NUM_STAGES: 3
    PADDING_KV: [1, 1, 1]
    PADDING_Q: [1, 1, 1]
    PATCH_PADDING: [2, 1, 1]
    PATCH_SIZE: [7, 3, 3]
    PATCH_STRIDE: [4, 2, 2]
    POS_EMBED: [False, False, False]
    QKV_BIAS: [True, True, True]
    QKV_PROJ_METHOD: ['dw_bn', 'dw_bn', 'dw_bn']
    STRIDE_KV: [2, 2, 2]
    STRIDE_Q: [1, 1, 1]
MODEL_SUMMARY: False
MULTIPROCESSING_DISTRIBUTED: True
NAME: cvt-21-384x384
OUTPUT_DIR: OUTPUT/
PIN_MEMORY: True
PRINT_FREQ: 500
RANK: 0
TEST:
  BATCH_SIZE_PER_GPU: 32
  CENTER_CROP: False
  IMAGE_SIZE: [384, 384]
  INTERPOLATION: 3
  MODEL_FILE: 
  REAL_LABELS: False
  VALID_LABELS: 
TRAIN:
  AUTO_RESUME: True
  BATCH_SIZE_PER_GPU: 128
  BEGIN_EPOCH: 0
  CHECKPOINT: 
  CLIP_GRAD_NORM: 0.0
  DETECT_ANOMALY: False
  END_EPOCH: 300
  EVAL_BEGIN_EPOCH: 0
  GAMMA1: 0.99
  GAMMA2: 0.0
  IMAGE_SIZE: [384, 384]
  LR: 0.00025
  LR_SCHEDULER:
    ARGS:
      cooldown_epochs: 10
      decay_rate: 0.1
      epochs: 300
      min_lr: 1e-05
      sched: cosine
      warmup_epochs: 5
      warmup_lr: 1e-06
    METHOD: timm
  MOMENTUM: 0.9
  NESTEROV: True
  OPTIMIZER: adamW
  OPTIMIZER_ARGS:
    
  SAVE_ALL_MODELS: False
  SCALE_LR: True
  SHUFFLE: True
  WD: 0.1
  WITHOUT_WD_LIST: ['bn', 'bias', 'ln']
VERBOSE: True
WORKERS: 6
2024-04-26 01:20:59,536:[P:8463]:Rank[0/2] => using 2 GPUs
2024-04-26 01:20:59,536:[P:8463]:Rank[0/2] => saving config into: OUTPUT/imagenet/cvt-21-384x384/config.yaml
2024-04-26 01:20:59,556:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,557:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,557:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,557:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,557:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,558:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,558:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,558:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,558:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,558:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,559:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,559:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,583:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,583:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,584:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,584:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,584:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,585:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,585:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,585:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,585:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,587:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,587:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,588:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,588:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,589:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,589:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,589:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,589:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,590:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,590:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,591:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,591:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,592:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,592:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,593:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,594:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,594:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,594:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,595:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,595:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,595:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,595:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,596:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,596:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,597:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,597:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,598:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,599:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,599:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,599:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,600:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,600:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,600:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,600:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,601:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,601:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,602:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,602:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,604:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,867:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,868:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,868:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,870:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,870:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,871:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,871:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,872:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,873:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,877:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,877:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,881:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,881:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,883:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,883:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,884:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,884:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,885:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,886:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,887:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,887:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,891:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,891:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,896:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,896:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,897:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,897:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,898:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,899:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,900:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,900:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,901:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,901:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,906:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,906:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,910:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,910:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,912:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,912:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,913:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,913:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,914:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,915:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,916:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,916:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,920:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,920:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,924:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,925:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,926:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,926:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,927:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,927:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,929:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,929:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,930:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,930:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,935:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,935:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,939:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,939:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,940:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,941:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,942:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,942:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,943:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,943:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,945:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,945:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,949:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,949:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,953:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,954:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,955:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,955:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,956:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,956:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,958:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,958:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,959:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,959:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,963:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,964:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,968:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,968:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,969:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,969:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,971:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,971:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,972:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,972:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,974:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,974:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,978:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,978:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,982:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,982:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,984:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,984:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,985:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,985:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,986:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,987:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,988:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,988:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,992:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,992:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,997:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,997:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:20:59,998:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:20:59,998:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,000:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,000:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,001:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,001:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,003:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,003:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,007:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,007:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,011:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,012:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,013:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,013:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,014:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,014:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,016:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,016:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,017:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,017:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,022:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,022:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,026:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,026:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,027:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,028:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,029:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,029:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,030:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,030:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,032:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,032:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,036:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,036:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,040:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,041:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,042:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,042:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,043:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,043:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,045:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,045:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,046:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,046:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,051:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,051:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,055:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,055:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,056:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,057:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,058:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,058:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,059:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,059:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,061:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,061:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,065:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,065:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,069:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,070:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,071:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,071:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,072:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,073:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,074:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,074:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,075:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,075:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,080:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,080:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,084:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,084:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,085:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,086:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,087:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,087:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,088:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,088:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,090:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,090:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,094:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,094:[P:8463]:Rank[0/2] => init weight of Linear from trunc norm
2024-04-26 01:21:00,100:[P:8463]:Rank[0/2] => init bias of Linear to zeros
2024-04-26 01:21:00,240:[P:8463]:Rank[0/2] => ConvolutionalVisionTransformer(
  (stage0): VisionTransformer(
    (patch_embed): ConvEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=64, out_features=64, bias=True)
          (proj_k): Linear(in_features=64, out_features=64, bias=True)
          (proj_v): Linear(in_features=64, out_features=64, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (stage1): VisionTransformer(
    (patch_embed): ConvEmbed(
      (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0-3): 4 x Block(
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=192, out_features=192, bias=True)
          (proj_k): Linear(in_features=192, out_features=192, bias=True)
          (proj_v): Linear(in_features=192, out_features=192, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (stage2): VisionTransformer(
    (patch_embed): ConvEmbed(
      (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.007)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.013)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.020)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.027)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.033)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.040)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.047)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.053)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.060)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.067)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.073)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.080)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.087)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.093)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (conv_proj_q): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_k): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (conv_proj_v): Sequential(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (rearrage): Rearrange('b c h w -> b (h w) c')
          )
          (proj_q): Linear(in_features=384, out_features=384, bias=True)
          (proj_k): Linear(in_features=384, out_features=384, bias=True)
          (proj_v): Linear(in_features=384, out_features=384, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.100)
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): QuickGELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=384, out_features=1000, bias=True)
)
2024-04-26 01:21:00,299:[P:8463]:Rank[0/2] Trainable Model Total Parameter: 	31.6M
2024-04-26 01:21:00,399:[P:8463]:Rank[0/2] => use timm transform for training
